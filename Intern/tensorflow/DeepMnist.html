<!DOCTYPE html>
<html lnag="ja">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <script type="text/javascript" src="../../js/syntaxhighlighter/scripts/shCore.js"></script>
    <script type="text/javascript" src="../../js/syntaxhighlighter/scripts/shBrushPowerShell.js"></script>
    <script type="text/javascript" src="../../js/syntaxhighlighter/scripts/shBrushPython.js"></script>
    <link type="text/css" rel="stylesheet" href="../../js/syntaxhighlighter/styles/shCoreDefault.css">
    <link type="text/css" rel="stylesheet" href="../../js/syntaxhighlighter/styles/shCore.css">
    <script type="text/javascript">
        SyntaxHighlighter.all()
    </script>
    <link rel="stylesheet" href="../../mdl-templates/templates/text-only/material.min.css">
    <script src="../../mdl-templates/templates/text-only/material.min.css"></script>
    <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
    <link rel="stylesheet" href="../../css/text.css">
</head>

<body>
    <div class="wrapper">
        <header>
            <div id="Logo">
                <div id="LogoImage">
                </div>
                <div id="LogoName">
                    Ogihara Akihiro
                </div>
            </div>
            <div id="breadcrumb">
            </div>
        </header>
        <main>
            2017/04/19
            <a href="https://www.tensorflow.org/get_started/mnist/pros">参考サイト</a>
            <div class="mainVisual">
                <p>MNISTとは:手書きの数字が大量に収録されているデータセット</p>
                TensorFlowは大規模な数的演算をすることが非常に得意です. このチュートリアルではdeep convolutional MNIST classiier(深層畳み込みMNIST分類器)を構築することをとおしてTensorFlowモデルのbasic
                building blocksを学びます.
                <h3>About this Tutorial</h3>
                このチュートリアルでははじめにmnist_softmax.py（よく使われるTensorFlowのモデル）のコードで何が起きているのかをご説明します.(ソフトマックス関数は機械学習でよく出てくる). 次のパートでaccuracy(分類精度)を向上させる方法をいくつかお教えします.
                このチュートリアルでは以下のことを目的としています.
                <ul>
                    <li>画像のすべての画素を見て文字の識別を行うsoftmax regression function(ソフトマックス回帰関数)を作れる様になること</li>
                    <li>数千枚の画像サンプルをmodel(モデル)に'見せ'て文字識別を行える様に訓練することをTensorFlowで行う.</li>
                    <li>テストデータを使いmodelの分類精度を確かめる</li>
                    <li>結果の改善のためにCNN(convolutional neural network)のbuild,trainそしてtestを行う</li>
                </ul>
                <h3>SetUp</h3>
                まずモデル構築どうこうの前に画像データが必要になるので,それを読み込みましょう.
                <h5>Load MNIST Data</h5>
                下のコードをpythonで打ちこめばかってにMNISTをロードしてくれます.
                <pre class="brush:python">
                   from tensorflow.examples.tutorials.mnist import input_data
                   mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
               </pre> ここの
                <strong>mnist</strong>はtraining,validation,そしてtestingのセットがnumpy配列として保管されている軽めのクラスである. ミニバッチごとに関数を生成することもできるので下のほうでそれをつかっています.
                <h5>Start TensorFlow InteractiveSession</h5>
                TensorFlowはその計算のバックエンドでC++の言語をつかっています. その間の繋がりがsessionと呼ばれます. tensorFlowの使い方の基本が
                <ol>
                    <li>まずgraph(グラフ)を作る.</li>
                    <li>それをsession(セッション)に移す</li>
                </ol>
                ここではTensorFlowでのコードの整形をより柔軟にするクラスの<strong>InteactiveSession</strong>というクラスを使います. これを使うとcomputation graphをbuildするoperation(操作)の中にgraphをrunするoperationを挟み込めます(?????).
                これはIpythonなどのインタラクティブな環境では非常に便利です. 普通ならすべてのcomputation graphをsessionの始まる前にbuildしなければいけないのです.
                <pre class="brush:python">
                   import tensorflow as tf
                   sess = tf.InteractiveSession()
               </pre>
                <h5>Computing Graph</h5>
                Pythonで効率の良い計算をするのには,NumPyのようなライブラリをつかいます. このライブラリは行列の乗法などの質の高い操作を,他の言語で実装された効率の良いコードによって行ってくれます. 残念なことにPythonに返す段階でオーバーヘッドが発生してしまっているのが現状ですが.
                GPUで計算する場合などにはこのオーバーヘッドが非常に良くないです.
                <br> tensorFlowとPythonのデータの交換も非常に重いが,こちらはオーバーヘッドを避けるステップを踏んでいる. Pythonに依存した質の高い操作を一つするのではなく,tensrFlowはPythonのそとで互いに鑑賞する操作をするのです(???)
                このアプローチはTheanoやTorchに似ています.
                <br> したがってPythonの役割はこの外部のcomputation graphのbuildとrunさせるべきcomputation graphへの指示になります.
                <h3>Build a Softmax Regression Model</h3>
                このセクションでは一つのlinear layerでsoftmax regression model(ソフトマックス回帰モデル)をbuildする. 次のセクションで,そのモデルをmultilayer convolutional network(多層畳み込みネットワーク)用に拡張します.
                <h5>Placeholder</h5>
                まずはinput image(入力画像)へのノード,output classes(出力クラス)へのノードを作成するところからcomputation graphの構築を始めましょう.
                <pre class="brush:python">
                   x = tf.placeholder(tf.float32, shape=[None,784])
                   y_ = tf.placeholder(tf.float32, shape=[none,10])
               </pre> どちらも
                <strong>placeholder</strong>なのでTensorflowでrunした時にその値を吐き出します.
                <br> まず入力画像のxは2次元のfloating point number(浮動小数点)のtenosr(行列)で構成されます. 今回,xには[None,784]のshape(次元)を割り当てたが,まずこの,
                <strong>784</strong>という値は,データセットの画像がもともと28×28でありそれを平らにしたデータを使うからである.
                <strong>None</strong>ははじめの次元であることを示します. batch_sizeと対応していていくつでも構いません. targetのy_のクラスもまた2次元の行列でできています. これはMNISTの各データに与えられた正解データ(0~9)が入れられます.
                <br> placeholderにおける
                <strong>shape</strong>引数はオプションですが,指定すると合わないshapeの行列にたいしてバグをキャッチしてくれます.
                <h5>Variable</h5>
                weights(重み)としてwをbiases(バイアス)としてbを定義します. これはイメージとしてはinputに追加すればいいのではとおもいますが,TensorFlowが提供する
                <strong>Variable</strong>を使うのがより良いでしょう.
                <strong>Variable</strong>はcomputation graphに密接します. これはcomputationでつかわれ,また修正されます. 機械学習アプリケーションでは,一般にモデルパラメータを持つ要素は
                <strong>Variable</strong>になります.
                <pre class="brush:python">
                    W = tf.Variable(tf.zeros[784,10])
                    b = tf.Variable(tf.zeros[10])
                </pre>
                <strong>tf.Variable</strong>の呼び出しでの各パラメータの初期化はどちらの値も0にすることでパスしています.
                Wは784×10の行列です.(input(入力)が784列でoutput(出力)が10列).
                そしてbが10の次元のvector(ベクトル)(出力が10クラスだから).
                <br>
                Variableがsesionで使われる前に,そのsessionをつかって初期値（今回ならすべてzeroで与えられている）で初期化(init)される必要がある.
                初期化されて初めて値を与えられる.
                すべてのVariableは以下で初期化される.
                <pre class = "brush:python">
                    sess.run(tf.global_variables_initializer())
                </pre>
                <h5>Predicted Class and Loss Function(クラスの推定と損失関数)</h5>
                回帰モデルの実装をできます.(というかします).
                onput image(入力画像)xをベクトル化した値に重み行列Wをかけて最期にバイアスbを加える一行入れるだけ.
                <pre class = "brush:python">
                    y = tf.matmul(x,W) + b
                </pre>
                loss function(損失関数)は簡単に定義できます.
                lossというのはあるひとつのexampleについてmodelのprediction(予測)がどのくらい悪いのかを表します.
                training(訓練)が終わるまでにすべてのexampleのlossが最小に去るようにします.
                我々のloss functionは対象とモデルの予測にソフトマックス関数を適用した値の間のcross-entropyになっています.
                初心者のチュートリアルとしては,下の安定した式
                <pre class = "brush:python">
                    cross_entropy = tf.reduce_mean(
                        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))
                </pre>
                <strong>tf.nn.softmax_cross_entropy_with_logits</strong>は内部で最適化されていないmodelのpredictionとすべてのクラスの和にたいしてsoftmaxを適用します.
                <strong>tf.reduce_mean</strong>はすべての和の平均をとります.
                <h5>Train the Model</h5>
                modelとloss functionの定義はできました.
                これはTensorFlowをつかったstraightforward(順伝搬)のtrainを行います.
                TensorFlowはcomputation graphの全体を知っているので,
                各Variable(変数)へのrequest(要求)からloss（損失)のgradient(勾配)を見つけることで,自動の分類を行えます.
                TensorFlowは様々な<a href="https://www.tensorflow.org/api_guides/python/train#optimizers">built-in optimization algorithm(最適化アルゴリズム)</a>
                を持ちます.
                今回はsteepest gradient descent(最急降下法）をstep length :0.5,descend:cross_entropyでつかっています.
                <pre class="brush:python">
                    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
                </pre>
                この一文でTensorFlowがなにをしているかというと,computation graphに新しいoperationを追加してるのです.
                そのoperationというのは,gradient(勾配)の計算,parameterの更新値の計算のstep,そして更新をparameterに適用するstepです.
                <br>
                run(実行)時に<strong>train_step</strong>から返されるoperatioｎはparameterを勾配降下です.
                そのためmodelのtrainingは<strong>train_step</strong>がrun(実行)されるたび行われます.
                <pre class ="brush:python">
                    for _ in range(1000):
                        batch = mnist.train.next_batch(100)
                        train_step.run(feed_dict={x: batch[0], y_: batch[1]})
                </pre>
                これはtraining iteration(訓練の繰り返し)ごとに100のtraining exampleを読み込んでいます.
                次の行で<strong>train_step</strong>をrunし,feed_dictをつかって,batch[0]に画像データ,batch[1]に正解データが入っているのでそれをplaceholderで定義したx,y_に
                渡しています.
                <strong>feed_dict</strong>はplaceholderに限らず,computation graph上のどんな行列でも使うことができます.

                <h5>Evaluate Model</h5>
                作ったmodelの精度はどうか.<br>
                まず,predicted label(予想ラベル)は,どこから出すのかですが,<strong>tf.argmax</strong>を使いましょう.
                これは入力のうち最も高いもののみを出力とします.
                例えば<br>
                <p>a = [0.1,0.2,0.5,0.3]<br>
                であるならば<br>
                tf.argmax(a,1)=[0,0,1,0]
                </p>
                となります.
                正解率を出すなら以下の用に書いてください.
                <pre class = "brush:python">
                    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
                </pre>
                modelの予想ｙの出力と正解ラベルy_それぞれをargmaxした時,その値が同じなら,1を返し,違うのであれば0を返します.<br>
                正解率はこれ
                <pre class = "brush:python">
                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
                </pre>
                やっと実行できます.evaluationをして確かめましょう.
                <pre class = "brush:python">
                    print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
                </pre>
                <h3>Build a Multilayer Convolutional Network</h3>
                92%なんていう精度はMNISTの分類結果としては最悪です.
                ということで,さっきのシンプルなモデルを改造して程よく高度なネットワーク,convolutional neuralnetwork(畳み込みネットワーク)を作って見ましょう.
                おそらく98%くらいの精度はでるのではないでしょうか.
                それくらいでれば最高ではないが,最悪でもないと言えるでしょう.
                <h5>Weight Initialization(重みの初期化)</h5>
                そのようなモデルの制作には,重みとバイアスを大量に使います.
                そうすると対象性の崩れからちゃんと0のgradient(勾配)になりません.
                ReLuのニューロンをつかっているので,微小な正の数で初期化する手もあるでしょう.
                今回はそれとは違う方法として,手動でVariableを生成する関数を作ってそれをつかうことにしましょう.
                <pre class = "brush:python">
                def weight_variable(shape):
                    initial = tf.truncated_normal(shape, stddev=0.1)
                    return tf.Variable(initial)
                
                def bias_variable(shape):
                   initial = tf.constant(0.1, shape=shape)
                   return tf.Variable(initial)
                </pre>

                <h5>Convolution and Pooling</h5>
                tensorFlowはConvolution(畳み込み)とPooling(プーリング)を柔軟に行います.
                boundaries(境界)やstride(ストライド:フィルタを適用する間隔)なんかも自由に設定できます.
                今回のexampleでは最も一般的なタイプに設定します.
                畳み込みフィルタリングはstride:1にゼロパディング(周囲に画素数0の画素を足すこと),よって出力はinputと同じサイズになります.
                poolingは2×2の大きさでmaxプーリング行う.
                codeをきれいに保つために,このようにoperationを抽象化しておきましょう.
                <pre class = "brush:python">
                   def conv2d(x, W):
                      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
                   def max_pool_2x2(x):
                      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                      strides=[1, 2, 2, 1], padding='SAME') 
                </pre>
                <h5>First Convolutional Layer(はじめの畳込み層)</h5>
                はじめの層の実装をします.
                ここはconvolution層(畳み込み層)の構成で,max poolingに繋がります.
                フィルタのサイズを表すpatch(パッチ)は5×5で,このフィルタで３２個の特徴量(これが出力になる)にします（28から増えている分はゼロパディングのぶんかな?）.
                重み行列でこれを表そうとすると,<strong>[5,5,1,32]</strong>になります.
                はじめの２つの値がpatch size(パッチサイズ)で5,5です.
                次の値がinput channel(入力チャンネル)で今回使うMNISTの画像はグレースケールなので
                1です.
                最期の値がoutput channel(出力チャンネル)で32です.
                またoutput channel(出力チャネル)の成分に添えられるbias(バイアス)も定義します.
                <pre class="brush:python">
                    W_conv1 = weight_variable([5, 5, 1, 32])
                    b_conv1 = bias_variable([32])
                </pre>
                定義した畳み込み層を使うには入力側のshape(行列の形)も変換する必要があります.
                MNISTははじめ<strong>[None,784]</strong>だったのですが,これを<strong>[-1,28,28,1]</strong>に変換します.
                2列目,3列目の値が画像の幅,高さに対応します(元の画像が28×28).
                4列目の値がcolor channelです.(グレー画像なので1,colorなら3)
                <pre class="brush:python">
                    x_image = tf.reshape(x, [-1,28,28,1])
                </pre>
                各値が定義できたので,入力画像<strong>x_image</strong>をweight tensor(重み行列,フィルタ)<strong>W_conv1</strong>で畳み込み,
                bias(バイアス)<strong>b_conv1</strong>を加え,活性化関数ReLuに投げます.
                最期にその出力をmax pooling<strong>max_pool_2×2</strong>メソッドに入力します.
                maxpool_2×2で出力画像は14×14になります.
                <h5>Second convolutional Layer(ふたつ目の畳込み層)</h5>
                deep networkを構築するためにいくつかの層を積み重ねます.
                ふたつ目の層は5×5のpatch(パッチ)のフィルタごとに64の特徴量を出力します.
                <pre class="brush:python">
                    W_conv2 = weight_variable([5, 5, 32, 64])
                    b_conv2 = bias_variable([64])
                    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
                    h_pool2 = max_pool_2x2(h_conv2)
                </pre>
                <h5>Densely Connected Layer(全結合層)</h5>
                いまh_pool2から出てくる画像は７×７のサイズになっています.
                出てきた出力のすべてを入力にした1024個のニューロンでできた層（全結合層）に渡します.
                h_pool2からの出力の行列を行列のbatchに突っ込みます.
                <pre class="brush:python">
                    W_fc1 = weight_variable([7 * 7 * 64, 1024])
                    b_fc1 = bias_variable([1024])
                    
                    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
                    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
                </pre>


                



            </div>
            <div class="globalNav">
            </div>
        </main>
        <footer>
            <aside id="sideBar">
            </aside>
            </aside>
        </footer>
    </div>
</body>

</html>